{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12)Program to Split sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\t\t\tsplitting \n",
      "  ['How to tokenize?\\n\\n\\n\\n\\n\\nLike $$\\t a boss.', 'Google is_ accessible via http://www.google.com', '1000 new followers! a #TwitterFamous']\n",
      "This is first type of splitting\n",
      "\n",
      "\n",
      "\n",
      "['How', 'to', 'tokenize?', 'Like', '$$', 'a', 'boss.']\n",
      "['Google', 'is_', 'accessible', 'via', 'http://www.google.com']\n",
      "['1000', 'new', 'followers!', 'a', '#TwitterFamous']\n"
     ]
    }
   ],
   "source": [
    "#(12a)Program to Split sentences\n",
    "lines=[\n",
    "'How to tokenize?\\n\\n\\n\\n\\n\\nLike $$\\t a boss.',\n",
    "'Google is_ accessible via http://www.google.com',\n",
    "'1000 new followers! a #TwitterFamous'\n",
    "]\n",
    "\n",
    "print('Before\\t\\t\\tsplitting \\n ',lines)\n",
    "print('This is first type of splitting\\n\\n\\n')\n",
    "# It will split 'How to tokenize?\\nLike $$\\t a\n",
    "#boss.' to 'How', 'to', 'tokenize?', 'Like', '$$', 'a', 'boss.'\n",
    "for line in lines:\n",
    "    print(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is first type\n",
      "\n",
      "\n",
      "\n",
      "['India', 'has', 'many', 'historical', 'monuments']\n",
      "['Many', 'information', '#can', 'be', '%obtained', 'from', 'http://www.google.com']\n",
      "['1000', 'new', 'followers!', 'a', '#TwitterFamous']\n"
     ]
    }
   ],
   "source": [
    "#(12b and c) Program to Split sentences\n",
    "lines2=[\n",
    "'India has many historical\\n\\n\\n\\n\\n\\n monuments\\t',\n",
    "'Many information\\t\\t\\t\\t\\t #can be %obtained from http://www.google.com',\n",
    "'1000 new followers! a #TwitterFamous'\n",
    "]\n",
    "\n",
    "print('This is first type\\n\\n\\n')\n",
    "for line in lines2:\n",
    "    print(line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Second Program for Splitting sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Regular Expressions (RegEx) is a special sequence of characters,that uses a search pattern to find a string or set of strings.\n",
    "It can detect the presence or absence of a text by matching it with a particular pattern, and also can split a pattern into one or more sub-patterns. \n",
    "Python provides a re module that supports the use of regex in Python. \n",
    "Its primary function is to offer a search, where it takes a regular expression and a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES\n",
      "this is the second type \n",
      "\n",
      "\n",
      "['H', 'o', 'w', 't', 'o', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'L', 'i', 'k', 'e', 'a', 'b', 'o', 's', 's']\n",
      "['G', 'o', 'o', 'g', 'l', 'e', 'i', 's', '_', 'a', 'c', 'c', 'e', 's', 's', 'i', 'b', 'l', 'e', 'v', 'i', 'a', 'h', 't', 't', 'p', 'w', 'w', 'w', 'g', 'o', 'o', 'g', 'l', 'e', 'c', 'o', 'm']\n",
      "['1', '0', '0', '0', 'n', 'e', 'w', 'f', 'o', 'l', 'l', 'o', 'w', 'e', 'r', 's', 'a', 'T', 'w', 'i', 't', 't', 'e', 'r', 'F', 'a', 'm', 'o', 'u', 's']\n"
     ]
    }
   ],
   "source": [
    "#(13a) Program to Split sentences\n",
    "print('THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES')\n",
    "lines=[\n",
    "'How to tokenize?\\nLike a boss.',\n",
    "'Google is_ accessible via http://www.google.com',\n",
    "'1000 new %followers! a #TwitterFamous'\n",
    "]\n",
    "\n",
    "import re\n",
    "_token_pattern=r'\\w'\n",
    "'''\n",
    "Pythonâ€™s re.compile() method is used to compile a regular expression\n",
    "pattern provided as a string into a regex pattern object (re.Pattern).\n",
    "#Later we can use this pattern object to search for a match inside \n",
    "different target strings using regex methods.\n",
    "'''\n",
    "token_pattern = re.compile(_token_pattern)\n",
    "\n",
    "print('this is the second type \\n\\n')\n",
    "for line in lines:\n",
    "    print(token_pattern.findall(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'\\w' for each character\n",
    "'\\w+' for each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Second Program for splitting sentences\n",
      "THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES\n",
      "The given sentence \n",
      " ['India is great']\n",
      "This is second type\n",
      "\n",
      "['India', 'is', 'great']\n"
     ]
    }
   ],
   "source": [
    "#(13b and c) Second Program for Splitting sentences\n",
    "print('2. Second Program for splitting sentences')\n",
    "print('THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES')\n",
    "lines=[\n",
    "'India is great'\n",
    "]\n",
    "\n",
    "import re\n",
    "_token_pattern= r'\\w+'\n",
    "token_pattern=re.compile(_token_pattern)\n",
    "\n",
    "print('The given sentence \\n',lines)\n",
    "print('This is second type\\n')\n",
    " \n",
    "for line in lines:\n",
    "    print(token_pattern.findall(line))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 third Program for splitting sentences\n",
      "THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES\n",
      "The given sentence \n",
      " ['India is great']\n",
      "This is second type\n",
      "\n",
      "['I', 'n', 'd', 'i', 'a', 'i', 's', 'g', 'r', 'e', 'a', 't']\n"
     ]
    }
   ],
   "source": [
    "#(13d)  Program for Splitting sentences\n",
    "print('3 third Program for splitting sentences')\n",
    "print('THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES')\n",
    "lines=[\n",
    "'India is great'\n",
    "]\n",
    "\n",
    "import re\n",
    "_token_pattern= r'\\w'\n",
    "token_pattern=re.compile(_token_pattern)\n",
    "\n",
    "print('The given sentence \\n',lines)\n",
    "print('This is second type\\n')\n",
    " \n",
    "for line in lines:\n",
    "    print(token_pattern.findall(line))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4th Program for splitting sentences\n",
      "THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES\n",
      "This is second type\n",
      "\n",
      "\n",
      "\n",
      "['How', 'to', 'tokenize', 'Like', 'a', 'boss']\n",
      "['Google', 'is_', 'accessible', 'via', 'http', 'www', 'google', 'com']\n",
      "['1000', 'new', 'followers', 'a', 'TwitterFamous']\n"
     ]
    }
   ],
   "source": [
    "#(13e) Second Program for Splitting sentences\n",
    "print('4th Program for splitting sentences')\n",
    "print('THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES')\n",
    "lines=[\n",
    "    'How to tokenize?\\nLike a boss.',\n",
    "    'Google is_ accessible via http://www.google.com',\n",
    "    '1000 new %followers! a #TwitterFamous'\n",
    "]\n",
    "\n",
    "\n",
    "import re\n",
    "_token_pattern= r'\\w+'# removes $$ , #, %, ?\n",
    "token_pattern=re.compile(_token_pattern)\n",
    "\n",
    "print('This is second type\\n\\n\\n')\n",
    "for line in lines:\n",
    "    print(token_pattern.findall(line))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14)Program to split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program for splitting sentences\n",
      "THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES\n",
      "This is second type\n",
      "\n",
      "\n",
      "\n",
      "['How', 'to', 'tokenize', 'Like', 'boss']\n",
      "['Google', 'is_', 'accessible', 'via', 'http', 'www', 'google', 'com']\n",
      "['1000', 'new', 'followers', 'TwitterFamous']\n"
     ]
    }
   ],
   "source": [
    "#(14a) Program for Splitting sentences\n",
    "print('Program for splitting sentences')\n",
    "print('THIS IS THE SECOND PROGRAM FOR SPLITTING SENTENCES')\n",
    "lines=[\n",
    "    'How to tokenize?\\nLike a boss.',\n",
    "    'Google is_ accessible via http://www.google.com',\n",
    "    '1000 new %followers! a #TwitterFamous'\n",
    "]\n",
    "\n",
    "\n",
    "import re\n",
    "#_token_pattern=r\"(?u)\\b\\w\\w+\\b\"\n",
    "#_token_pattern=r\"\\b\\w\\w+\\b\"\n",
    "#_token_pattern=r\"\\w\\w+\\b\"\n",
    "_token_pattern= r'\\w\\w+'# removes $$ , #, %, ? and single letter words like a\n",
    "token_pattern=re.compile(_token_pattern)\n",
    "\n",
    "print('This is second type\\n\\n\\n')\n",
    "for line in lines:\n",
    "    print(token_pattern.findall(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Program to split sentences where one character words are not considered\n",
      "THIS TYPE-3\n",
      "The given sentences  ['India is a #great country']\n",
      "This is third type\n",
      "\n",
      "\n",
      "['India', 'is', 'great', 'country']\n"
     ]
    }
   ],
   "source": [
    "#(14b)Program to split sentences\n",
    "print(' Program to split sentences where one character words are not considered')\n",
    "print('THIS TYPE-3')\n",
    "lines=[\n",
    "'India is a #great country'\n",
    "]\n",
    "import re\n",
    "_token_pattern=r\"\\w\\w+\"\n",
    "#_token_pattern=r\"\\w+\"\n",
    "token_pattern=re.compile(_token_pattern)\n",
    "print('The given sentences ',lines)\n",
    "print('This is third type\\n\\n')\n",
    "for line in lines:\n",
    "    print(token_pattern.findall(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15)Program to split sentences where hashtag and url are\n",
    "represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The re.sub() function is a method provided by the Python re (regular expressions) module for performing string substitution based on regular expression patterns. It allows you to search for a pattern within a given string and replace all occurrences of that pattern with a specified replacement string.\n",
    "\n",
    "syntax: re.sub(pattern, repl, input_string, count=0, flags=0)\n",
    "\n",
    "pattern: This is the regular expression pattern you want to search for in the input string.\n",
    "\n",
    "repl: This is the replacement string that will be used to replace the matched pattern.\n",
    "\n",
    "string: This is the input string in which you want to perform the substitution.\n",
    "\n",
    "count (optional): It specifies the maximum number of replacements to perform. If omitted or set to 0, all occurrences of the pattern are replaced.\n",
    "\n",
    "flags (optional): These are optional flags that modify the behavior of the regular expression. For example, you can use the re.IGNORECASE flag to perform a case-insensitive search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Program to split sentences and replace hashtag and url are represented with strings\n",
      "THIS IS THE FOURTH TYPE\n",
      "The given data\n",
      "\n",
      "\n",
      " ['How to #tokenize?\\nLike a %boss.', 'Google @is_ $accessible @via http://www.google.com.com.abc', 'https://abc.ybc.cef', '$abc,$edfg', '1000 10new #followers! a #TwitterFamous $TwitterFamous']\n",
      "this the fourth type\n",
      "\n",
      "\n",
      "['how', 'to', '_hashtag_', 'like', 'a', '_percent_']\n",
      "['google', 'is_', 'accessible', 'via', '_url_']\n",
      "['_url_']\n",
      "['abc', 'edfg']\n",
      "['_num_', '_num_new', '_hashtag_', 'a', '_hashtag_', 'twitterfamous']\n"
     ]
    }
   ],
   "source": [
    "#(15a) Program to split sentences\n",
    "print(' Program to split sentences and replace hashtag and url are represented with strings')\n",
    "print('THIS IS THE FOURTH TYPE')\n",
    "lines=[\n",
    "'How to #tokenize?\\nLike a %boss.',\n",
    "'Google @is_ $accessible @via http://www.google.com.com.abc',\n",
    "'https://abc.ybc.cef',\n",
    "'$abc,$edfg',\n",
    "'1000 10new #followers! a #TwitterFamous $TwitterFamous'\n",
    "]\n",
    "print('The given data\\n\\n\\n',lines)\n",
    "#for line in lines:\n",
    "# print(line.split())\n",
    "import re\n",
    "_token_pattern=r\"\\w+\"\n",
    "token_pattern=re.compile(_token_pattern)\n",
    "\n",
    "# preprocesses and tokenizes a text line by lowercasing it, \n",
    "# replacing URLs, hashtags, percentages, and numeric values with placeholders, \n",
    "# and then tokenizing the resulting text \n",
    "def tokenizer(line):\n",
    "    line=line.lower() #  converts the input line to lowercase. This can help ensure consistent tokenization. \n",
    "    #to find and replace URLs in the line with the string '_url_'.\n",
    "    line=re.sub(r'http[s]?://[\\w\\.\\?]+','_url_',line) #matches URLs that start with \"http://\" or \"https://\" followed by word characters, dots, and optional question marks.\n",
    "    #to find and replace hashtags in the line with the string '_hashtag_'\n",
    "    line=re.sub(r'#\\w+','_hashtag_',line)#matches hashtags that start with \"#\" followed by one or more word characters.\n",
    "    # to find and replace percentage values in the line with the string '_percent_'\n",
    "    line=re.sub(r'%\\w+','_percent_',line)#matches percentage values that start with \"%\" followed by one or more word characters.\n",
    "    #to find and replace numeric values in the line with the string '_num_'\n",
    "    line=re.sub(r'\\d+','_num_',line)# matches one or more digits.\n",
    "    return token_pattern.findall(line)\n",
    "\n",
    "print(\"this the fourth type\\n\\n\")\n",
    "\n",
    "for line in lines:\n",
    "    print(tokenizer(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program to split the words and converts to lower case and removes Special characters and http[s]\n",
      "The given  ['How to tokenize?\\nLike a boss.', 'Google is_ accessible via http://www.google.com', 'Google is_ accessible via http://www.google.com.com.com.com', 'Google is_ accessible via https://www.google.com.com.com.com', '1000 new followers! a #TwitterFamous']\n",
      "This is fourth type\n",
      "\n",
      "\n",
      " It converts to lower case and removes Special characters and http[s]\n",
      "['how', 'to', 'tokenize', 'like', 'a', 'boss']\n",
      "['google', 'is_', 'accessible', 'via', 'url']\n",
      "['google', 'is_', 'accessible', 'via', 'url']\n",
      "['google', 'is_', 'accessible', 'via', 'url']\n",
      "['1000', 'new', 'followers', 'a', 'twitterfamous']\n"
     ]
    }
   ],
   "source": [
    "#(15b)\n",
    "print('Program to split the words and converts to lower case and removes Special characters and http[s]')\n",
    "lines=[\n",
    "'How to tokenize?\\nLike a boss.',\n",
    "'Google is_ accessible via http://www.google.com',\n",
    "'Google is_ accessible via http://www.google.com.com.com.com',\n",
    "'Google is_ accessible via https://www.google.com.com.com.com',\n",
    "'1000 new followers! a #TwitterFamous'\n",
    "]\n",
    "print('The given ',lines)\n",
    "\n",
    "import re\n",
    "_token_pattern=r\"\\w+\"\n",
    "token_pattern=re.compile(_token_pattern)\n",
    "def tokenizer(line):\n",
    "    line= line.lower()\n",
    "    line=re.sub(r'http[s]?://[\\w\\.\\?]+','url',line)\n",
    "    return token_pattern.findall(line)\n",
    "\n",
    "print('This is fourth type\\n')\n",
    "print('\\n It converts to lower case and removes Special characters and http[s]')\n",
    "for line in lines:\n",
    "    print(tokenizer(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16) Program on Count Vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16a and d)\n",
    "The code demonstrates how to use CountVectorizer for tokenization and word counting, with the added functionality of customizing the tokenization process to handle lowercase, URLs, hashtags, and numeric values. The resulting DataFrame xyz contains the word counts for each unique word in the input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Program on CountVectorizer to tokenize the given sentences\n",
      "THIS IS THE PROGRAM ON CountVectorizer\n",
      "ONLY Count Vectorizer\n",
      "input lines after regex cleaning: \n",
      "\n",
      "['how', 'to', 'tokenize', 'like', 'a', 'boss']\n",
      "['google', 'google', 'is_', 'accessible', 'via', '_url_', 'google', 'com']\n",
      "['_num_', 'new', 'followers', 'a', '_hastag_']\n",
      "\n",
      "x.todense = <bound method spmatrix.todense of <3x16 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 17 stored elements in Compressed Sparse Row format>>\n",
      "\n",
      "It is the result of xyz=pd.DataFrame:\n",
      "\n",
      "   _hastag_  _num_  _url_  a  accessible  boss  com  followers  google  how  \\\n",
      "0         0      0      0  1           0     1    0          0       0    1   \n",
      "1         0      0      1  0           1     0    1          0       3    0   \n",
      "2         1      1      0  1           0     0    0          1       0    0   \n",
      "\n",
      "   is_  like  new  to  tokenize  via  \n",
      "0    0     1    0   1         1    0  \n",
      "1    1     0    0   0         0    1  \n",
      "2    0     0    1   0         0    0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#(16a and d) Program on Count Vectorizer\n",
    "\n",
    "#import lib\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(' Program on CountVectorizer to tokenize the given sentences')\n",
    "print('THIS IS THE PROGRAM ON CountVectorizer')\n",
    "print('ONLY Count Vectorizer')\n",
    "lines=[\n",
    "'How to tokenize?\\nLike a boss.',\n",
    "'Google google is_ accessible via http://www.google.com',\n",
    "'1000 new followers! a #TwitterFamous'\n",
    "]\n",
    "\n",
    "def tokenizer(line):\n",
    "    line=line.lower()\n",
    "    line=re.sub(r'http[s]?://[\\w\\?]+','_url_',line)\n",
    "    line=re.sub(r'#\\w+','_hastag_',line)\n",
    "    line=re.sub(r'\\d+','_num_',line)\n",
    "    return token_pattern.findall(line)\n",
    "\n",
    "_token_pattern=r'\\w+'\n",
    "token_pattern=re.compile(_token_pattern)\n",
    "\n",
    "print(\"input lines after regex cleaning: \\n\")\n",
    "for line in lines:\n",
    "    print(tokenizer(line))\n",
    "\n",
    "#initializes a CountVectorizer instance with specific settings:\n",
    "#lowercase=True: Converts all text to lowercase.\n",
    "#tokenizer=tokenizer: Specifies the custom tokenizer function to use for tokenization.\n",
    "vec=CountVectorizer(lowercase=True, tokenizer=tokenizer)\n",
    "#appling the fit_transform method of the CountVectorizer \n",
    "#to the lines of text, which tokenizes and counts the words in the sentences\n",
    "x=vec.fit_transform(lines)\n",
    "#x is a sparse matrix that represents the tokenized and counted words from the input sentences\n",
    "#print(\"\\n x= \\n\",x)\n",
    "xyz=pd.DataFrame(\n",
    "    x.todense(),#converts the sparse matrix x into a dense matrix.\n",
    "    columns=vec.get_feature_names_out()# sets the column names of the DataFrame to represent the unique words in the text data.\n",
    ")\n",
    "\n",
    "# print information about the dense matrix and set options to display all rows and columns in the DataFrame.\n",
    "print('\\nx.todense =',x.todense)\n",
    "pd.set_option(\"display.max_rows\",None)\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "\n",
    "#print the DataFrame itself, which contains the tokenized and counted words from the input sentences.\n",
    "print('\\nIt is the result of xyz=pd.DataFrame:\\n')\n",
    "print(xyz)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result of creating a DataFrame xyz from the sparse matrix x. Each row represents one of the input sentences, and each column represents a unique word or token found in the sentences. The values in the DataFrame represent the word counts for each word in each sentence. For example, in the first row and column _hashtag_, the value is 0, indicating that the word _hashtag_ is not present in the first sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 b)\n",
    "Setting tokenizer=None in the CountVectorizer is useful when you don't want any custom tokenization or text preprocessing to be applied to your text data. When tokenizer=None, the CountVectorizer will use its default tokenization strategy, which is to split the text on whitespace and apply some basic cleaning like converting text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentences: \n",
      " ['Flights was much  delayed, I am not happy much', 'Flight was not delayed, I am happy']\n",
      "\n",
      "word count in each row:\n",
      "    am  delayed  flight  flights  happy  much  not  was\n",
      "0   1        1       0        1      1     2    1    1\n",
      "1   1        1       1        0      1     0    1    1\n"
     ]
    }
   ],
   "source": [
    "#(16b)\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "flight_delayed_lines=[\n",
    "    'Flights was much  delayed, I am not happy much',\n",
    "    'Flight was not delayed, I am happy'\n",
    "]\n",
    "\n",
    "def tokenizer(line):\n",
    "    line=line.lower()\n",
    "    line=re.sub(r'http[s]?//:[\\w\\?]+','url',line)\n",
    "    line=re.sub(r'#\\w+','hastag',line)\n",
    "    line=re.sub(r'\\d+','num',line)\n",
    "    return token_pattern.findall(line)\n",
    "\n",
    "vec=CountVectorizer(lowercase=True, tokenizer=None) #setting tokenizer=None ,to usedefault tokenization strategy\n",
    "x=vec.fit_transform(flight_delayed_lines)\n",
    "\n",
    "xyz=pd.DataFrame(\n",
    "    x.todense(),\n",
    "    columns=vec.get_feature_names_out()\n",
    ")\n",
    "print(\"input sentences: \\n\",flight_delayed_lines)\n",
    "print(\"\\nword count in each row:\\n\",xyz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.todense :\n",
      " [[1 1 1 1 1 1 1]\n",
      " [1 1 1 1 0 1 1]]\n",
      "\n",
      "x.todense size : (2, 7)\n",
      "\n",
      "xyz\n",
      "    am  delayed  flight  happy  much  not  was\n",
      "0   1        1       1      1     1    1    1\n",
      "1   1        1       1      1     0    1    1\n"
     ]
    }
   ],
   "source": [
    "#(16c)\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "flight_delayed_lines=[\n",
    "    'Flight was  delayed, I am not happy much',\n",
    "    'Flight was not delayed, I am happy'\n",
    "]\n",
    "\n",
    "vec=CountVectorizer(lowercase=True)\n",
    "x=vec.fit_transform(flight_delayed_lines)\n",
    "\n",
    "xyz=pd.DataFrame(\n",
    "    x.todense(),\n",
    "    columns=vec.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(\"x.todense :\\n\",x.todense())\n",
    "print(\"\\nx.todense size :\",x.todense().shape)\n",
    "print(\"\\nxyz\\n\",xyz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17) Program on TfidfTransformer\n",
    "TfidfTransformer: Transform a count matrix to a normalized tf or tfidf representation.\n",
    "Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency.This is a common term weighting scheme in information retrieval,that has also found good use in document classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of text vectorization and natural language processing, a \"document\" typically refers to a single unit of text that can be considered as a standalone piece of content.\n",
    "In the provided example, \"apple apple\" and \"apple orange\" are treated as documents, although they are quite short. In practice, documents can vary in length and complexity, and the goal of text analysis is often to process, analyze, and extract meaningful information from these documents. Text vectorization techniques like TF-IDF or Count Vectorization help convert these documents into numerical representations that machine learning models can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "   (0, 0)\t1.0\n",
      "  (1, 1)\t0.8148024746671689\n",
      "  (1, 0)\t0.5797386715376657\n",
      "\n",
      "learned vocabulary:\n",
      " {'apple': 0, 'orange': 1}\n",
      "\n",
      "converting sparse to dense array:\n",
      " [[1.         0.        ]\n",
      " [0.57973867 0.81480247]]\n"
     ]
    }
   ],
   "source": [
    "#17a)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvec=TfidfVectorizer()\n",
    "documents=[\n",
    "    \"apple apple\",\n",
    "    \"apple orange\"\n",
    "]\n",
    "\n",
    "x=tvec.fit_transform(documents)#vectorizing documents\n",
    "print(\"x:\\n\",x)\n",
    "\n",
    "print(\"\\nlearned vocabulary:\\n\",tvec.vocabulary_) # printing learned vocab of vectorizer\n",
    "print(\"\\nconverting sparse to dense array:\\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output demonstrates the TF-IDF vectorization of two simple documents (\"apple apple\" and \"apple orange\") using the learned vocabulary.The learned vocabulary from the TF-IDF vectorization consists of two terms: \"apple\" and \"orange.\"\n",
    "These terms are assigned numerical indices for vectorization, with \"apple\" assigned index 0 and \"orange\" assigned index 1., resulting in a sparse matrix (x) and its dense representation, where each term's TF-IDF score is calculated for each document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
