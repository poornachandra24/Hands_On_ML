{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Program 2\n",
    "Program on Word 2 vector and cosine similarity\n",
    "\n",
    "\n",
    "Cosine similarity is a metric used to measure how similar two vectors are in a multi-dimensional space. It is particularly popular in natural language processing and information retrieval for comparing the similarity of documents or words based on their vector representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vector for apples:\n",
      "  0    -1.244198\n",
      "1     0.849777\n",
      "2    -0.847986\n",
      "3     1.536878\n",
      "4     1.451894\n",
      "        ...   \n",
      "91    0.708207\n",
      "92    1.898031\n",
      "93   -0.110315\n",
      "94   -0.203278\n",
      "95    0.668344\n",
      "Name: apples, Length: 96, dtype: float64\n",
      "\n",
      "cosine similarity matrix :\n",
      "              I   like  apples  oranges  pears\n",
      "I        1.000  0.163   0.376    0.168 -0.088\n",
      "like     0.163  1.000   0.073   -0.006  0.217\n",
      "apples   0.376  0.073   1.000    0.750  0.289\n",
      "oranges  0.168 -0.006   0.750    1.000  0.584\n",
      "pears   -0.088  0.217   0.289    0.584  1.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy #natural language processing (NLP) library in Python.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#loads an English language model called \"en_core_web_sm\" from spaCy using spacy.load(). \n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "#This model provides pre-trained word embeddings and other linguistic information for English text.\n",
    "\n",
    "terms=['I','like','apples','oranges','pears']\n",
    "\n",
    "#spaCy's nlp() function to obtain its word embedding vector using .vector. \n",
    "# .tolist() converts this vector to a Python list\n",
    "vectors=[\n",
    "    nlp(term).vector.tolist() for term in terms\n",
    "]\n",
    "\n",
    "# the word vector for 'apples' is extracted from the vectors list using terms.index('apples') \n",
    "# to find the index of 'apples' in the terms list. \n",
    "x=pd.Series(vectors[terms.index('apples')]).rename('apples')\n",
    "print(\"word vector for apples:\\n \",x)#prints the Pandas Series x, which represents the word vector for 'apples'.\n",
    "\n",
    "# a Pandas DataFrame named abc is created\n",
    "abc=pd.DataFrame(\n",
    "    cosine_similarity(vectors),\n",
    "    index=terms,\n",
    "    columns=terms\n",
    ").round(3)\n",
    "\n",
    "#prints the cosine similarity matrix stored in the abc DataFrame. \n",
    "# The matrix shows how similar each term is to every other term in the terms list based on their word embeddings.\n",
    "print(\"\\ncosine similarity matrix :\\n\",abc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value at index 0 is approximately -1.244198, and the value at index 1 is approximately 0.849777. These values indicate the position of the word \"apples\" in a high-dimensional space, where each dimension represents some aspect of the word's meaning.Word vectors are often used to capture semantic information about words, allowing algorithms to understand and work with word meanings in a numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Program to dataset from url \n",
    "\n",
    "downloading and reading sentiment analysis data from UCI's Machine Learning Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libs\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "#Data Directory Creation\n",
    "data_dir=f'{os.getcwd()}/data'\n",
    "#It checks if a directory named 'data' exists in the current working directory (os.getcwd()). \n",
    "# If not, it creates the 'data' directory.\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "\n",
    "import requests\n",
    "url =\"https://archive.ics.uci.edu/static/public/331/sentiment+labelled+sentences.zip\"\n",
    "response = requests.get(url)\n",
    "\n",
    "#extracts the contents of the downloaded zip file\n",
    "#BytesIO(response.content) converts the binary content into an in-memory binary stream using BytesIO.\n",
    "with zipfile.ZipFile(file=BytesIO(response.content),mode='r') as compressed_file:\n",
    "    compressed_file.extractall(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assuming that the CSV files are located in a subdirectory named 'sentiment labelled sentences' within the previously created data_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>This is a stunning movie.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>I had the mac salad and it was pretty bland so I will not be getting that again.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The food, amazing.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Audio Quality is poor, very poor.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>His acting alongside Olivia De Havilland was brilliant and the ending was fantastic!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>The shrimp tender and moist.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Was not happy.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>The headsets are easy to use and everyone loves them.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>It is wonderful and inspiring to watch, and I hope that it gets released again on to v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>The problem I have is that they charge $11.99 for a sandwich that is no bigger than a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          text  \\\n",
       "471                                                                This is a stunning movie.     \n",
       "278           I had the mac salad and it was pretty bland so I will not be getting that again.   \n",
       "20                                                                          The food, amazing.   \n",
       "150                                                          Audio Quality is poor, very poor.   \n",
       "430     His acting alongside Olivia De Havilland was brilliant and the ending was fantastic!     \n",
       "39                                                                The shrimp tender and moist.   \n",
       "340                                                                             Was not happy.   \n",
       "84                                       The headsets are easy to use and everyone loves them.   \n",
       "599  It is wonderful and inspiring to watch, and I hope that it gets released again on to v...   \n",
       "984  The problem I have is that they charge $11.99 for a sandwich that is no bigger than a ...   \n",
       "\n",
       "     sentiment  \n",
       "471          1  \n",
       "278          0  \n",
       "20           1  \n",
       "150          0  \n",
       "430          1  \n",
       "39           1  \n",
       "340          0  \n",
       "84           1  \n",
       "599          1  \n",
       "984          0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialized an empty list called df_list, \n",
    "# will be used to store DataFrames read from individual CSV files.\n",
    "df_list = []\n",
    "for csv_file in ['imdb_labelled.txt','yelp_labelled.txt','amazon_cells_labelled.txt']:\n",
    "    # constructs the full file path for each CSV file using f'{data_dir}/sentiment labelled sentences/{csv_file}'\n",
    "    csv_file_with_path = f'{data_dir}/sentiment labelled sentences/{csv_file}'\n",
    "    temp_df = pd.read_csv(\n",
    "                            csv_file_with_path,\n",
    "                            sep=\"\\t\",#tab-separated \n",
    "                            header=0,#indicates that the first row contains column names.\n",
    "                            names=['text', 'sentiment'] #provides column names for the DataFrame.\n",
    "    )\n",
    "    df_list.append(temp_df)# resulting DataFrame (temp_df) for each CSV file is appended to the df_list.\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "pd.options.display.max_colwidth = 90\n",
    "df[['text', 'sentiment']].sample(10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Program on Naïve Bayes Classifier\n",
    "\n",
    "The Naïve Bayes Classifier is a probabilistic machine learning algorithm that is used for classification tasks, particularly in natural language processing (NLP) and text classification. It's based on Bayes' theorem and is considered \"naïve\" because it makes a simplifying assumption that the features (or attributes) used to describe data are independent of each other, which may not always hold true in real-world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train,df_test=train_test_split(df,test_size=0.4,random_state=42)\n",
    "\n",
    "y_train=df_train['sentiment']\n",
    "y_test=df_test['sentiment']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction: It uses CountVectorizer to convert the text data into numerical features. \n",
    "\n",
    "All values of n such such that min_n <= n <= max_n will be used.\n",
    "For example an ngram_range of (1, 1) means only unigrams,\n",
    "(1, 2) means unigrams and bigrams, and (2, 2) means only bigrams.\n",
    "Only applies if analyzer is not callable.\n",
    "For example, setting the ngram_range to 2, 2 will return bigrams (2-grams) or two word phrases.\n",
    "\n",
    "The ngram_range parameter (1,3) specifies that both unigrams (single words) and trigrams (sequences of three words) should be considered as features.\n",
    "\n",
    "The min_df parameter sets a minimum document frequency of 3 for a word to be included as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec=CountVectorizer(ngram_range=(1,3),min_df=3,strip_accents='ascii')\n",
    "# x_train and x_test are vectors\n",
    "x_train=vec.fit_transform(df_train['text'])\n",
    "x_test=vec.transform(df_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a Naïve Bayes classifier, you need labeled data where each data point is associated with a known class label. The classifier calculates probabilities during training and uses them during inference to assign a class label to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf=MultinomialNB(fit_prior=True)\n",
    "clf.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.79       565\n",
      "           1       0.77      0.80      0.79       533\n",
      "\n",
      "    accuracy                           0.79      1098\n",
      "   macro avg       0.79      0.79      0.79      1098\n",
      "weighted avg       0.79      0.79      0.79      1098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,precision_recall_fscore_support\n",
    "print(classification_report(y_test, y_test_pred)) #precision    recall  f1-score   support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precison:  [0.80586081 0.77355072]\n",
      "recall:  [0.77876106 0.8011257 ]\n",
      "fscore:  [0.79207921 0.78709677]\n",
      "support:  [565 533]\n"
     ]
    }
   ],
   "source": [
    "p,r,f,s= precision_recall_fscore_support(y_test,y_test_pred)\n",
    "print(\"precison: \",p)\n",
    "print(\"recall: \", r)\n",
    "print(\"fscore: \",f)\n",
    "print(\"support: \",s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
